{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Squared Error and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement gradient descent in Python code. Using the sigmoid as the activation function f(h), implement gradient descent for the following input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input data\n",
    "x = np.array([[0.1, 0.3]])\n",
    "# Target\n",
    "y = np.array([0.2])\n",
    "\n",
    "# Input to output weights\n",
    "weights = np.array([-0.8, 0.5])\n",
    "\n",
    "# The learning rate, eta in the weight step equation\n",
    "learnrate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.1 0.3]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_points(X, y):\n",
    "    admitted = X[np.argwhere(y==0.2)]\n",
    "    print(admitted)\n",
    "    rejected = X[np.argwhere(y==0)]\n",
    "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')\n",
    "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')\n",
    "    \n",
    "plot_points(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement gradient descent as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid funtion\n",
    "def sigmoid_prime(x):\n",
    "    return sigmod(x) * (1 - sigmoid(x))\n",
    "\n",
    "# the linear combination performed by the node (h in f(h) and f'(h))\n",
    "h = x[0][0]*weights[0] + x[0][1]*weights[1]\n",
    "# or h = np.dot(x,weights)\n",
    "\n",
    "# The neural network ouput (y-hat)\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# output error (y - y-hat)\n",
    "error = y - nn_output\n",
    "\n",
    "# output gradient (f'(h))\n",
    "output_grad = sigmoid_prime(h)\n",
    "\n",
    "# error term (lowercase delta)\n",
    "error_term = error * output_grad\n",
    "\n",
    "# Gradient descent step\n",
    "del_w = [ learnrate * error_term * x[0][0],\n",
    "          learnrate * error_term * x[0][1]]\n",
    "# or del_w = learnrate * error_term * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn_output: 0.5174928576663897\n",
      "error: [-0.31749286]\n",
      "output_grad: 0.24969399993066344\n",
      "error_term: [-0.07927606]\n",
      "del_w: [array([-0.0039638]), array([-0.01189141])]\n"
     ]
    }
   ],
   "source": [
    "print(\"nn_output:\", nn_output)\n",
    "print(\"error:\", error)\n",
    "print(\"output_grad:\", output_grad)\n",
    "print(\"error_term:\", error_term)\n",
    "print(\"del_w:\", del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll implement gradient descent again, but the data now has more than 2 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement gradient descent as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.6899744811276125\n",
      "Amount of Error:\n",
      "-0.1899744811276125\n",
      "Change in Weights:\n",
      "[-0.02031869 -0.04063738 -0.06095608 -0.08127477]\n"
     ]
    }
   ],
   "source": [
    "### Calculate one gradient descent step for each weight\n",
    "### Note: Some steps have been consolidated, so there are\n",
    "###       fewer variable names than in the above sample code\n",
    "\n",
    "# TODO: Calculate the node's linear combination of inputs and weights\n",
    "h = np.dot(x, w)\n",
    "\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# TODO: Calculate the error term\n",
    "#       Remember, this requires the output gradient, which we haven't\n",
    "#       specifically added a variable for.\n",
    "error_term = error * sigmoid_prime(h)\n",
    "# Note: The sigmoid_prime function calculates sigmoid(h) twice,\n",
    "#       but you've already calculated it once. You can make this\n",
    "#       code more efficient by calculating the derivative directly\n",
    "#       rather than calling sigmoid_prime, like this:\n",
    "# error_term = error * nn_output * (1 - nn_output)\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
